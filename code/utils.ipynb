{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "drawn-stone",
   "metadata": {},
   "source": [
    "## tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "elementary-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "import re\n",
    "import argparse\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 문장 하나에 대해 processing하는 함수 클래스\n",
    "class EntityPosMarker:\n",
    "    \"\"\" 한국어 문장 하나에 대해 entity의 위치를 special token으로 나타낸 후,\n",
    "        tokenize하여 BERT에 들어갈 수 있는 input ids 형태로 변환.\n",
    "    \n",
    "    Attributes:\n",
    "        tokenizer: BertTokenizer(bert-base-multilingual-cased model을 사용).\n",
    "        args: Args from command line.\n",
    "    \"\"\"\n",
    "    def __init__(self, args=None):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.err = 0\n",
    "        self.args = args\n",
    "    \n",
    "    def tokenize(self, sentence, subj_pos_range, obj_pos_range):\n",
    "        \"\"\" Entity Marker를 special token 형태로 추가한 후 BERT-input ids로 변환하는 함수.\n",
    "\n",
    "        Args:\n",
    "            sentence: 한국어 문장 (type: str)\n",
    "            subject_pos_range: subject entity의 위치 인덱스 범위 (인덱스는 문자열 인덱스)\n",
    "            object_pos_range: object entity의 위치 인덱스 범위 (인덱스는 문자열 인덱스)\n",
    "\n",
    "        Returns:\n",
    "            tokenized_input_ids: BERT-input ids\n",
    "            subj_marker_start: subject entity marker 시작 위치 인덱스\n",
    "            subj_marker_end: subject entity marker 끝 위치 인덱스\n",
    "            obj_marker_start: object entity marker 시작 위치 인덱스\n",
    "            obj_marker_end: object entity marker 끝 위치 인덱스\n",
    "\n",
    "        Example:\n",
    "            sentence: \"한국은 동아시아의 한반도에 위치하고 있다.\"\n",
    "            subj_pos_range: [0, 2] (\"한국\")\n",
    "            obj_pos_range: [4, 8] (\"동아시아\")\n",
    "\n",
    "            1. tokenizer.tokenize(sentence)\n",
    "                tokens = ['한국', '##은', '동', '##아', '##시아', '##의', '한', '##반', '##도에', '위', '##치', '##하고', '있다', '.']\n",
    "            2. special token 추가\n",
    "                tokenized_sentence = ['[CLS]', '[unused1]', '한국', '[unused2]', '##은', '[unused3]', '동', '##아', '##시아', \n",
    "                                    '[unused4]', '##의', '한', '##반', '##도에', '위', '##치', '##하고', '있다', '.', '[SEP]']\n",
    "            3. tokenized sentence를 BERT-input ids로 변환 및 entities 위치 찾아 return\n",
    "                tokenized_input_ids = [101, 1, 48556, 2, 10892, 3, 9095, 16985, 46861, 4, 10459, 9954, 30134, 108521,\n",
    "                                    9619, 18622, 12453, 11506, 119, 102]\n",
    "        \"\"\"\n",
    "        # subj_name, obj_name\n",
    "        subj_name = sentence[subj_pos_range[0]:subj_pos_range[1]]\n",
    "        obj_name = sentence[obj_pos_range[0]:obj_pos_range[1]]\n",
    "\n",
    "        subj_start_idx, subj_end_idx = [], []\n",
    "        obj_start_idx, obj_end_idx = [], []\n",
    "\n",
    "        # 1. tokenizer.tokenize(sentence)\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "\n",
    "        # subj, obj token 위치 찾기\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token[0] == subj_name[0]:\n",
    "                subj_start_idx.append(i)\n",
    "            if token[-1] == subj_name[-1]:\n",
    "                subj_end_idx.append(i)\n",
    "        \n",
    "            if token[0] == obj_name[0]:\n",
    "                obj_start_idx.append(i)\n",
    "            if token[-1] == obj_name[-1]:\n",
    "                obj_end_idx.append(i)\n",
    "        \n",
    "        # subj token idx\n",
    "        subj_flag = False\n",
    "        for i in subj_start_idx:\n",
    "            for j in subj_end_idx:\n",
    "                # tmp_tokens = ['한국']\n",
    "                tmp_tokens = tokens[i:j+1]\n",
    "                subj_cand = \" \".join(tmp_tokens)\n",
    "                subj_cand = re.sub(\" ##\", \"\", subj_cand)\n",
    "                if subj_cand == subj_name:\n",
    "                    subj_token_start, subj_token_end = i, j+1\n",
    "                    subj_flag = True\n",
    "                    break\n",
    "            if subj_flag:\n",
    "                break\n",
    "\n",
    "        # obj token idx\n",
    "        obj_flag = False\n",
    "        for i in obj_start_idx:\n",
    "            for j in obj_end_idx:\n",
    "                # tmp_tokens = ['동', '##아', '##시아']\n",
    "                tmp_tokens = tokens[i:j+1]\n",
    "                obj_cand = \" \".join(tmp_tokens)\n",
    "                obj_cand = re.sub(\" ##\", \"\", obj_cand)\n",
    "                if obj_cand == obj_name:\n",
    "                    obj_token_start, obj_token_end = i, j+1\n",
    "                    obj_flag = True\n",
    "                    break\n",
    "            if obj_flag:\n",
    "                break\n",
    "\n",
    "        # subj_tokens = tokens[subj_token_start:subj_token_end]\n",
    "        # obj_tokens = tokens[obj_token_start:obj_token_end]\n",
    "\n",
    "        # 2. special token 추가\n",
    "        tokenized_sentence = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            if i == subj_token_start:\n",
    "                tokenized_sentence.append(\"[unused1]\")\n",
    "            elif i == obj_token_start:\n",
    "                tokenized_sentence.append(\"[unused3]\")\n",
    "\n",
    "            if i == subj_token_end:\n",
    "                tokenized_sentence.append(\"[unused2]\")\n",
    "            elif i == obj_token_end:\n",
    "                tokenized_sentence.append(\"[unused4]\")\n",
    "\n",
    "            tokenized_sentence.append(token)\n",
    "\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"]\n",
    "\n",
    "        # 3. entity marker token 위치 찾기\n",
    "        try:\n",
    "            subj_marker_start = tokenized_sentence.index(\"[unused1]\")\n",
    "            subj_marker_end = tokenized_sentence.index(\"[unused2]\")\n",
    "        except:\n",
    "            self.err += 1\n",
    "            subj_marker_start = 0\n",
    "            subj_marker_end = 2\n",
    "        \n",
    "        try:\n",
    "            obj_marker_start = tokenized_sentence.index(\"[unused3]\")\n",
    "            obj_marker_end = tokenized_sentence.index(\"[unused4]\")\n",
    "        except:\n",
    "            self.err += 1\n",
    "            obj_marker_start = 0\n",
    "            obj_marker_end = 2\n",
    "\n",
    "        tokenized_input_ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        return tokenized_input_ids, subj_marker_start, subj_marker_end, obj_marker_start, obj_marker_end\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-synthetic",
   "metadata": {},
   "source": [
    "## train data 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cheap-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open(\"../data/train.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        item = json.loads(line)\n",
    "        train_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fuzzy-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data의 개수: 2685657\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data의 개수: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conservative-peninsula",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': '/data/modified_KBN_data/wikipedia_0001.json',\n",
       " 'sentence': '제임스 얼 \"지미\"카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.',\n",
       " 'subj': {'name': '제임스 얼', 'pos': [0, 5], 'type': 'PS'},\n",
       " 'obj': {'name': '카터 주니어', 'pos': [10, 16], 'type': 'PS'},\n",
       " 'description': '항목 주제의 후임자',\n",
       " 'confidence': 0.8254553079605103}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "banner-million",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': '/data/modified_KBN_data/wikipedia_0001.json',\n",
       " 'sentence': '제임스 얼 \"지미\"카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.',\n",
       " 'subj': {'name': '제임스 얼', 'pos': [0, 5], 'type': 'PS'},\n",
       " 'obj': {'name': '미국', 'pos': [44, 46], 'type': 'LC'},\n",
       " 'description': '항목 주제의 국가',\n",
       " 'confidence': 0.5040757060050964}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-worthy",
   "metadata": {},
   "source": [
    "## EntityPosMarker 사용 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moving-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = EntityPosMarker()\n",
    "\n",
    "example = train_data[0]\n",
    "\n",
    "sentence = example['sentence']\n",
    "subj_pos_range = example['subj']['pos']\n",
    "obj_pos_range = example['obj']['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cordless-physics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 1, 9672, 36240, 12605, 9551, 2, 107, 9706, 22458, 107, 3, 9786, 21876, 9689, 25503, 12965, 4, 113, 117, 11416, 10954, 16650, 18329, 198, 114, 9043, 9311, 16323, 21928, 9768, 25387, 23545, 11303, 48506, 70672, 113, 10722, 10954, 198, 96318, 114, 30919, 119, 102], 1, 6, 11, 17)\n"
     ]
    }
   ],
   "source": [
    "print(marker.tokenize(sentence=sentence, subj_pos_range=subj_pos_range, obj_pos_range=obj_pos_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adverse-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input_ids, subj_marker_start, subj_marker_end, obj_marker_start, obj_marker_end = marker.tokenize(sentence, subj_pos_range, obj_pos_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "blank-permit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids: \n",
      "[101, 1, 9672, 36240, 12605, 9551, 2, 107, 9706, 22458, 107, 3, 9786, 21876, 9689, 25503, 12965, 4, 113, 117, 11416, 10954, 16650, 18329, 198, 114, 9043, 9311, 16323, 21928, 9768, 25387, 23545, 11303, 48506, 70672, 113, 10722, 10954, 198, 96318, 114, 30919, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input ids: \\n{tokenized_input_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mighty-registration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subj entity marker 위치: [1, 6]\n",
      "Obj entity marker 위치: [11, 17]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Subj entity marker 위치: [{subj_marker_start}, {subj_marker_end}]\")\n",
    "print(f\"Obj entity marker 위치: [{obj_marker_start}, {obj_marker_end}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "extraordinary-traveler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: 제임스 얼 \"지미\"카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.\n",
      "Subj name: 제임스 얼\n",
      "Obj name: 카터 주니어\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Sentence: {sentence}\")\n",
    "print(f\"Subj name: {sentence[subj_pos_range[0]:subj_pos_range[1]]}\")\n",
    "print(f\"Obj name: {sentence[obj_pos_range[0]:obj_pos_range[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "strategic-mozambique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[unused1]', '제', '##임', '##스', '얼', '[unused2]', '\"', '지', '##미', '\"', '[unused3]', '카', '##터', '주', '##니', '##어', '[unused4]', '(', ',', '1924', '##년', '10월', '1일', '~', ')', '는', '민', '##주', '##당', '출', '##신', '미국', '39', '##번째', '대통령', '(', '1977', '##년', '~', '1981년', ')', '이다', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(marker.tokenizer.convert_ids_to_tokens(tokenized_input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-aircraft",
   "metadata": {},
   "source": [
    "원래 문장이 tokenize된 후 entity marker의 역할을 하는 special token이 추가된 형태로 BERT-input ids로 변환되는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-beads",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
